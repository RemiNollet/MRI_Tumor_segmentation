{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3573fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da355a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -O https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/projects/60fb61/brats_2019.zip \n",
    "!unzip -q brats_2019.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4022e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2606b",
   "metadata": {},
   "source": [
    "# Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f9d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HGG_path = Path((\"data/MICCAI_BraTS_2019_Data_Training/HGG/\"))\n",
    "LGG_path = Path((\"data/MICCAI_BraTS_2019_Data_Training/LGG/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ef96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load NIfTI file from HGG\n",
    "img = nib.load(os.path.join(HGG_path, \"BraTS19_2013_2_1/BraTS19_2013_2_1_flair.nii\"))\n",
    "data = img.get_fdata()\n",
    "data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc87b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load NIfTI file from LGG\n",
    "img = nib.load(os.path.join(LGG_path, \"BraTS19_2013_0_1/BraTS19_2013_0_1_flair.nii\"))\n",
    "data = img.get_fdata()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d72491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot a slice\n",
    "plt.imshow(data[:, :, 80], cmap=\"gray\") # coupe selon z=80 -> faire une video avec toutes les hauteurs? Couper selon les autres axes? \n",
    "plt.title(\"Middle slice of brain MRI\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs = []\n",
    "\n",
    "# selectionner x patients au hasard\n",
    "x = 10\n",
    "patients = [p for p in HGG_path.iterdir() if p.is_dir()]\n",
    "sampled_patients = random.sample(patients, x)\n",
    "print(sampled_patients)\n",
    "\n",
    "# Boucle sur les x patients\n",
    "for patient in sampled_patients:\n",
    "    print(patient.name)\n",
    "    T1 = nib.load(f\"{HGG_path}/{patient.name}/{patient.name}_t1.nii\")\n",
    "    T1CE = nib.load(f\"{HGG_path}/{patient.name}/{patient.name}_t1ce.nii\")\n",
    "    T2 = nib.load(f\"{HGG_path}/{patient.name}/{patient.name}_t2.nii\")\n",
    "    FLAIR = nib.load(f\"{HGG_path}/{patient.name}/{patient.name}_flair.nii\")\n",
    "    SEG = nib.load(f\"{HGG_path}/{patient.name}/{patient.name}_seg.nii\")\n",
    "    imgs.append([T1, T1CE, T2, FLAIR, SEG])\n",
    "\n",
    "\n",
    "# Plot a slice\n",
    "fig, axes = plt.subplots(x, 5, figsize=(20, x*4))  # x rows, 5 columns\n",
    "\n",
    "for i, patient in enumerate(imgs):\n",
    "    for j, img in enumerate(patient):\n",
    "        data = img.get_fdata()\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(data[:, :, 80], cmap=\"gray\")\n",
    "        ax.set_title(f\"Patient {i+1} - Img {j+1}\")\n",
    "        ax.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14742f7",
   "metadata": {},
   "source": [
    "# Pre rocessing\n",
    "Nous avons des images en 240x240x155 et nous souhaitons les redimensionner en 240x240x144 (sans alterer les dimensions). \\\n",
    "Nous constatons que sur les grande majorite des images, le cerveau s'arrete a 147 pixels et commence a 3 pixels (sur l'axe z). \\\n",
    "Nous allons donc supprimer les 3 premieres couches ainsi que les 8 dernieres pour supprimer les couches vides et ainsi obtenir des parrallelepipedes de 240x240x144."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:,:,3:147]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7d940",
   "metadata": {},
   "source": [
    "## Save Dataset 2D dans un format .npy injectable dans tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6265a6f",
   "metadata": {},
   "source": [
    "- **Charger les volumes IRM FLAIR (`*_flair.nii`) et leurs segmentations associées (`*_seg.nii`) pour chaque patient HGG et LGG.**\n",
    "- **Appliquer un crop** sur l’axe Z (profondeur) pour ne conserver que les 144 coupes centrales :  \n",
    "  `volume[:, :, 3:147]` → dimensions finales : `(240, 240, 144)`\n",
    "- **Découper chaque volume 3D** en slices 2D (une par coupe selon Z).\n",
    "- **Sauvegarder chaque slice 2D au format `.npy`** dans la structure suivante :\n",
    "\n",
    "dataset_UNET_2D/ \\\n",
    "| \\\n",
    "|- X/ \\\n",
    "|   |- HGG_BraTS19_XXX_slice_000.npy \\\n",
    "|   |- LGG_BraTS19_YYY_slice_001.npy \\\n",
    "|   |- ... \\\n",
    "| \\\n",
    "|- Y/ \\\n",
    "|   |- HGG_BraTS19_XXX_slice_000.npy \\\n",
    "|   |- LGG_BraTS19_YYY_slice_001.npy \\\n",
    "|   |- ... \\\n",
    "\n",
    "Chaque image dans `X/` correspond à une coupe IRM FLAIR normalisée, et chaque image dans `Y/` est le masque de segmentation associé.\n",
    "\n",
    "Le dossier `dataset_UNET_2D/` constitué de slices 2D prétraitées (entrées FLAIR et masques) constitue la **base d’apprentissage pour le modèle U-Net**, en vue d’une **segmentation précise des zones tumorales sur chaque coupe IRM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548345f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins d’entrée\n",
    "HGG_path = Path(\"data/MICCAI_BraTS_2019_Data_Training/HGG\")\n",
    "LGG_path = Path(\"data/MICCAI_BraTS_2019_Data_Training/LGG\")\n",
    "\n",
    "# Chemins de sortie\n",
    "output_X_2D = Path(\"data/MICCAI_BraTS_2019_Data_Training/dataset_UNET_2D/X\")\n",
    "output_Y_2D = Path(\"data/MICCAI_BraTS_2019_Data_Training/dataset_UNET_2D/Y\")\n",
    "\n",
    "output_X_2D.mkdir(parents=True, exist_ok=True)\n",
    "output_Y_2D.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_patient(patient_dir, tumor_type):\n",
    "    try:\n",
    "        patient_id = patient_dir.name\n",
    "        flair_path = patient_dir / f\"{patient_id}_flair.nii\"\n",
    "        seg_path = patient_dir / f\"{patient_id}_seg.nii\"\n",
    "\n",
    "        flair_data = nib.load(flair_path).get_fdata()[:, :, 3:147]  # crop en Z\n",
    "        seg_data = nib.load(seg_path).get_fdata()[:, :, 3:147]      # crop en Z\n",
    "\n",
    "        # Générer des slices suivant Z\n",
    "        for i in range(flair_data.shape[2]):\n",
    "            img_slice = flair_data[:, :, i]\n",
    "            seg_slice = seg_data[:, :, i]\n",
    "\n",
    "            # Sauvegarde au format .npy\n",
    "            slice_name = f\"{tumor_type}_{patient_id}_slice_{i:03}\"\n",
    "            np.save(output_X_2D / f\"{slice_name}.npy\", img_slice)\n",
    "            np.save(output_Y_2D / f\"{slice_name}.npy\", seg_slice)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {patient_dir.name}: {e}\")\n",
    "\n",
    "# Appliquer à HGG et LGG\n",
    "for patient in HGG_path.iterdir():\n",
    "    if patient.is_dir():\n",
    "        process_patient(patient, tumor_type=\"HGG\")\n",
    "\n",
    "for patient in LGG_path.iterdir():\n",
    "    if patient.is_dir():\n",
    "        process_patient(patient, tumor_type=\"LGG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e10e4f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42ec8a",
   "metadata": {},
   "source": [
    "## UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39478ee",
   "metadata": {},
   "source": [
    "### Creation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ba721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "X_paths = sorted(glob.glob((\"data/MICCAI_BraTS_2019_Data_Training/dataset_UNET_2D/X/*.npy\")))\n",
    "Y_paths = sorted(glob.glob((\"data/MICCAI_BraTS_2019_Data_Training/dataset_UNET_2D/Y/*.npy\")))\n",
    "\n",
    "def load_npy_pair(x_path, y_path):\n",
    "    x = np.load(x_path.decode()).astype(np.float32)\n",
    "    y = np.load(y_path.decode()).astype(np.int32)\n",
    "\n",
    "    # Test de validité des classes du masque\n",
    "    # TEMP : Affiche les valeurs uniques du masque\n",
    "    # print(\"UNIQUE LABELS IN y:\", np.unique(y))\n",
    "    assert np.all(np.isin(y, [0, 1, 2, 4])), \"Masque invalide : valeurs inattendues dans y\"\n",
    "    # Remap classes 4 → 3\n",
    "    y[y == 4] = 3\n",
    "\n",
    "    # Normalisation min-max slice par slice\n",
    "    if x.max() > 0:  # éviter division par zéro\n",
    "        x = x / x.max()\n",
    "\n",
    "    # Remplacement des NaN éventuels\n",
    "    x = np.nan_to_num(x)\n",
    "\n",
    "    x = np.expand_dims(x, axis=-1)  # (240, 240, 1)\n",
    "    y = np.expand_dims(y, axis=-1)\n",
    "    return x, y\n",
    "\n",
    "def tf_wrapper(x_path, y_path):\n",
    "    x, y = tf.numpy_function(load_npy_pair, [x_path, y_path], [tf.float32, tf.int32])\n",
    "\n",
    "    # Définir manuellement les shapes pour TensorFlow\n",
    "    x.set_shape((240, 240, 1))\n",
    "    y.set_shape((240, 240, 1))\n",
    "    return x, y\n",
    "\n",
    "# Fonction de construction du dataset\n",
    "def make_dataset(data_pairs, batch_size=8):\n",
    "    X, Y = zip(*data_pairs)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((list(X), list(Y)))\n",
    "    ds = ds.map(tf_wrapper)\n",
    "    ds = ds.shuffle(100)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE) # pipeline asynchrone optimisé\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd64afb",
   "metadata": {},
   "source": [
    "### Creation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432c1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "def encoding_layer(input_layer,output_channels,kernel_size):\n",
    "    conv1 = Conv2D(output_channels, (kernel_size, kernel_size), activation='relu', padding='same',\\\n",
    "                   kernel_initializer=tf.random_normal_initializer(0, 0.02)) (input_layer)\n",
    "    return conv1\n",
    "\n",
    "def bottleneck_layer(input_layer,output_channels,kernel_size):\n",
    "    bottleneck1 = Conv2D(output_channels, (kernel_size, kernel_size), activation='relu', padding='same',\\\n",
    "                         kernel_initializer=tf.random_normal_initializer(0, 0.02)) (input_layer)\n",
    "    bottleneck2 = Conv2D(output_channels, (kernel_size, kernel_size), activation='relu', padding='same',\\\n",
    "                         kernel_initializer=tf.random_normal_initializer(0, 0.02)) (bottleneck1)\n",
    "    return bottleneck2\n",
    "\n",
    "def decoding_layer(input_layer,skip_layer,output_channels,kernel_size,stride):\n",
    "    upconv1 = Conv2DTranspose(output_channels,  (kernel_size, kernel_size),strides=(stride,stride), padding='same',\\\n",
    "                              kernel_initializer=tf.random_normal_initializer(0, 0.02)) (input_layer)\n",
    "    concat1 = concatenate([upconv1, skip_layer])\n",
    "    conv1 = Conv2D(output_channels, kernel_size, activation='relu', padding='same',\\\n",
    "                   kernel_initializer=tf.random_normal_initializer(0, 0.02)) (concat1)\n",
    "    return conv1\n",
    "\n",
    "def create_unet(input_shape=(240,240,1), num_classes=4):\n",
    "    inputs_coarse = Input(input_shape)\n",
    "\n",
    "    encoding_layer1=encoding_layer(inputs_coarse,64,3)\n",
    "    pool1 = MaxPooling2D((2, 2),padding='same') (encoding_layer1)\n",
    "    encoding_layer2=encoding_layer(pool1,128,3)\n",
    "    pool2 = MaxPooling2D((2, 2),padding='same') (encoding_layer2)\n",
    "    encoding_layer3=encoding_layer(pool2,256,3)\n",
    "    pool3 = MaxPooling2D((2, 2),padding='same') (encoding_layer3)\n",
    "    encoding_layer4=encoding_layer(pool3,512,3)\n",
    "    pool4 = MaxPooling2D((2, 2),padding='same') (encoding_layer4)\n",
    "\n",
    "    bottleneck=bottleneck_layer(pool4,1024,3)\n",
    "\n",
    "    decoding_layer1= decoding_layer(bottleneck,encoding_layer4,512,3,2)\n",
    "    decoding_layer2= decoding_layer(decoding_layer1,encoding_layer3,256,3,2)\n",
    "    decoding_layer3 = decoding_layer(decoding_layer2,encoding_layer2,128,3,2)\n",
    "    decoding_layer4 = decoding_layer(decoding_layer3,encoding_layer1,64,3,2)\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax') (decoding_layer4)\n",
    "\n",
    "    model = Model(inputs=inputs_coarse, outputs=[outputs])\n",
    "    optim=Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optim, loss=['sparse_categorical_crossentropy'], metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5862d0d1",
   "metadata": {},
   "source": [
    "### Training avec Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037d9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lancer dans un terminal la commande: tensorboard --logdir \"./logs\"\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "\n",
    "EPOCH = 3\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bdb296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Creation du dataset\n",
    "\n",
    "## Créer les paires\n",
    "data = list(zip(X_paths, Y_paths))\n",
    "random.shuffle(data)\n",
    "\n",
    "## Split Dataset Train, Val, Test\n",
    "n = len(data)\n",
    "train_data = data[:int(0.8*n)]\n",
    "val_data   = data[int(0.8*n):int(0.95*n)]\n",
    "test_data  = data[int(0.95*n):]\n",
    "\n",
    "# Reduction du datset pour debug\n",
    "# train_data = train_data[:len(train_data)//5]\n",
    "# val_data = val_data[:len(val_data)//5]\n",
    "test_data = test_data[:len(test_data)//2]\n",
    "\n",
    "train_ds = make_dataset(train_data, batch_size=BATCH_SIZE)\n",
    "val_ds   = make_dataset(val_data, batch_size=BATCH_SIZE)\n",
    "test_ds  = make_dataset(test_data, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457791a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525c9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation du model\n",
    "model = create_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e161ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "tensorboard_callback_unet = TensorBoard(log_dir=(\"logs/unet\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCH,\n",
    "    callbacks=[\n",
    "        tensorboard_callback_unet, \n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        ModelCheckpoint(\"checkpoints/unet_best.h5\", monitor='val_loss', save_best_only=True)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8504ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2007125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Load best model\n",
    "# Lien du model: https://drive.google.com/file/d/1HmYfTN_u3WyNwxJLp2943ULw1m2a0Ws4/view?usp=sharing \n",
    "\n",
    "loaded_model = load_model(\"checkpoints/unet_best.h5\")\n",
    "\n",
    "class_names = ['Fond', 'Nécrose', 'Œdème', 'Rehaussement']\n",
    "\n",
    "# Display FLAIR, ground truth and prediciton\n",
    "def display_prediction(model, dataset, class_names=class_names, num_samples=3):\n",
    "    \"\"\"\n",
    "    Affiche des échantillons (FLAIR, masque GT, prédiction) à partir du modèle et du dataset.\n",
    "\n",
    "    Args:\n",
    "        model: modèle Keras entraîné\n",
    "        dataset: tf.data.Dataset (ex: val_ds ou test_ds)\n",
    "        class_names: liste des noms de classes (ex: ['fond', 'nécrose', 'œdème', 'rehaussement'])\n",
    "        num_samples: nombre d’échantillons à afficher\n",
    "    \"\"\"\n",
    "\n",
    "    for x_batch, y_batch in dataset.take(1):\n",
    "        preds = model.predict(x_batch)\n",
    "\n",
    "        # Convertir softmax → classe prédite (par pixel)\n",
    "        preds_classes = np.argmax(preds, axis=-1)\n",
    "        y_true_classes = np.squeeze(y_batch.numpy(), axis=-1)\n",
    "        \n",
    "        # Limite au nombre réel d’images dans le batch\n",
    "        n = min(num_samples, x_batch.shape[0])\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            img = np.squeeze(x_batch[i].numpy(), axis=-1)\n",
    "            gt = y_true_classes[i]\n",
    "            pred = preds_classes[i]\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            titles = ['FLAIR slice', 'Ground truth mask', 'Predicted mask']\n",
    "\n",
    "            for j, data in enumerate([img, gt, pred]):\n",
    "                axes[j].imshow(data, cmap='gray' if j == 0 else 'nipy_spectral', vmin=0, vmax=3)\n",
    "                axes[j].set_title(titles[j])\n",
    "                axes[j].axis('off')\n",
    "\n",
    "            if class_names:\n",
    "                cmap = plt.get_cmap('nipy_spectral')\n",
    "                patches = [plt.plot([],[], marker=\"s\", ls=\"\", color=cmap(i/len(class_names)))[0]\n",
    "                           for i in range(len(class_names))]\n",
    "                plt.legend(patches, class_names, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "# Binarise output and add threshold to match evaluation metrics precision/recall\n",
    "def display_prediction_binarised(model, dataset, class_names=class_names,\n",
    "                       num_samples=3, thresh=0.30):\n",
    "    \"\"\"\n",
    "    Affiche FLAIR, masque Ground-Truth et prédiction binaire (seuil > `thresh`)\n",
    "    pour `num_samples` coupes issues du premier batch du dataset.\n",
    "    \"\"\"\n",
    "    for x_batch, y_batch in dataset.take(1):\n",
    "        preds = model.predict(x_batch, verbose=0)\n",
    "\n",
    "        # probabilité totale « tumeur »  = somme des classes 1-2-3\n",
    "        tumor_prob = preds[..., 1:].sum(axis=-1)\n",
    "\n",
    "        # Binarisation à `thresh`\n",
    "        pred_bin = (tumor_prob > thresh).astype(np.uint8)\n",
    "\n",
    "        # GT (binaire pareil) pour lecture plus claire\n",
    "        y_true_bin = (tf.squeeze(y_batch, axis=-1).numpy() > 0).astype(np.uint8)\n",
    "\n",
    "        n = min(num_samples, x_batch.shape[0])\n",
    "        for i in range(n):\n",
    "            img  = np.squeeze(x_batch[i].numpy(), axis=-1)\n",
    "            gt   = y_true_bin[i]\n",
    "            pred = pred_bin[i]\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            titles = ['FLAIR slice',\n",
    "                      'Ground-truth (binaire)',\n",
    "                      f'Predicted mask\\n(thresh {thresh})']\n",
    "\n",
    "            for ax, data, title in zip(axes,\n",
    "                                       [img, gt, pred],\n",
    "                                       titles):\n",
    "                ax.imshow(data,\n",
    "                          cmap='gray' if data.ndim == 2 and data.max() <= 1\n",
    "                                     else 'nipy_spectral',\n",
    "                          vmin=0, vmax=1)\n",
    "                ax.set_title(title)\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36ba8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_prediction(loaded_model, test_ds, num_samples=3)\n",
    "display_prediction_binarised(loaded_model, test_ds, num_samples=3, thresh=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23fcb8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53389f09-fcaf-4ae7-9a8e-da7e77720dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On verifie que le test dataset contient des valeurs\n",
    "print(sum(1 for _ in test_ds))\n",
    "for xb, yb in test_ds.take(1):\n",
    "    print(xb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492234d-578a-42e2-b28c-e5e84d68e885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_segmentation(model, dataset):\n",
    "    \"\"\"\n",
    "    Calcule Recall & Precision sans créer d’énormes listes.\n",
    "    Binarisation : tumeur = 1 (classes > 0), fond = 0.\n",
    "    \"\"\"\n",
    "    tp = fp = fn = 0   # compteurs globaux\n",
    "\n",
    "    for xb, yb in tqdm(dataset, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        preds = loaded_model.predict(xb, verbose=0)        # (B,240,240,4)\n",
    "        tumor_prob = preds[..., 1:].sum(axis=-1)            # prob(tumeur) ∈ [0,1]\n",
    "        pred_bin   = tumor_prob > 0.3                     # 0.25 = seuil plus bas\n",
    "        true_bin = (tf.squeeze(yb, axis=-1).numpy() > 0)    # bool (B,H,W)\n",
    "\n",
    "        tp += np.logical_and(pred_bin,  true_bin).sum()     # vrais positifs\n",
    "        fp += np.logical_and(pred_bin, ~true_bin).sum()     # faux positifs\n",
    "        fn += np.logical_and(~pred_bin,  true_bin).sum()    # faux négatifs\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        print(\"Aucun pixel tumoral dans le jeu d’éval.\")\n",
    "        return None, None\n",
    "\n",
    "    recall    = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "\n",
    "    print(f\"Recall   : {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    return recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59217f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall, precision = evaluate_segmentation(loaded_model, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a049ce-10c6-4475-9b51-9ee243214500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
